{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APEX - Mountain car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/raylogs/.\n",
      "Waiting for redis server at 127.0.0.1:55878 to respond...\n",
      "Waiting for redis server at 127.0.0.1:30080 to respond...\n",
      "Starting local scheduler with the following resources: {'CPU': 8, 'GPU': 1}.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui98800.ipynb?token=5b6a7fa6354bc850611e9e6c3c58694f4488a7b905d1cea3\n",
      "======================================================================\n",
      "\n",
      "Created LogSyncer for /root/ray_results/2018-07-02_11-39-349uiosgsf -> None\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Observation shape is (2,)\n",
      "Not using any observation preprocessor.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING: Serializing objects of type <class 'ray.tune.registry._Registry'> by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import ray\n",
    "import time\n",
    "\n",
    "#from ray.rllib.ppo import PPOAgent, DEFAULT_CONFIG\n",
    "#from ray.rllib.a3c import A3CAgent, DEFAULT_CONFIG\n",
    "#from ray.rllib.dqn import DQNAgent, DEFAULT_CONFIG\n",
    "\n",
    "from ray.rllib.dqn.apex import ApexAgent, APEX_DEFAULT_CONFIG\n",
    "#from ray.rllib.ddpg.apex import ApexDDPGAgent, APEX_DDPG_DEFAULT_CONFIG # no API so far\n",
    "\n",
    "\n",
    "# start ray library, 1G memory for object_store\n",
    "ray.init(num_workers=0,\n",
    "         num_cpus=8,\n",
    "         num_gpus=1,\n",
    "         object_store_memory=1073741824\n",
    "        )\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# Set RL Agent\n",
    "\n",
    "#config = DEFAULT_CONFIG.copy()\n",
    "config = APEX_DEFAULT_CONFIG.copy()\n",
    "\n",
    "# Ape-X dqn parameters\n",
    "#config[\"gpu\"] = False\n",
    "config[\"lr\"] = 2e-4\n",
    "config[\"num_workers\"] = 4\n",
    "config[\"buffer_size\"] = 50000\n",
    "config[\"learning_starts\"] = 500\n",
    "config[\"train_batch_size\"] = 128\n",
    "config[\"target_network_update_freq\"] = 5000\n",
    "\n",
    "# ppo alg parameters\n",
    "#config['num_sgd_iter'] = 64\n",
    "#config['sgd_batchsize'] = 256\n",
    "#config['model']['fcnet_hiddens'] = [100, 100]\n",
    "\n",
    "# set agent\n",
    "#agent = PPOAgent(config, 'CartPole-v0')\n",
    "#agent = A3CAgent(config, 'CartPole-v0')\n",
    "#agent = DQNAgent(config, 'CartPole-v0')\n",
    "\n",
    "#agent = ApexAgent(config, 'CartPole-v0')\n",
    "agent = ApexAgent(config, 'MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Redis database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using port: 30080\n",
      "{'maxmemory': '1073741824'}\n",
      "{'maxmemory-policy': 'allkeys-lru'}\n",
      "{'maxmemory-samples': '3'}\n"
     ]
    }
   ],
   "source": [
    "# Create a Redis client just for config Redis.\n",
    "import redis\n",
    "\n",
    "r_primary = ray.worker.global_worker.redis_client\n",
    "r_primary.keys(\"*\")\n",
    "running_port = int(str(r_primary.lrange('RedisShards', 0, -1)).split(\":\")[1][:-2])\n",
    "print (\"Using port: {}\".format(running_port))\n",
    "\n",
    "redis_client1 = redis.StrictRedis(host=\"127.0.0.1\", port=running_port)\n",
    "#redis_client2 = redis.StrictRedis(host=\"127.0.0.1\", port=16254)\n",
    "\n",
    "redis_client1.config_set(\"maxmemory\", \"1024mb\")  # 512 MB limit\n",
    "redis_client1.config_set(\"maxmemory-policy\", \"allkeys-lru\")\n",
    "redis_client1.config_set(\"maxmemory-samples\", \"3\")\n",
    "\n",
    "#redis_client2.config_set(\"maxmemory\", \"2048mb\")  # 512 MB limit\n",
    "#redis_client2.config_set(\"maxmemory-policy\", \"allkeys-lru\")\n",
    "#redis_client2.config_set(\"maxmemory-samples\", \"3\")\n",
    "\n",
    "# check setting result\n",
    "print(redis_client1.config_get(\"maxmemory\"))\n",
    "print(redis_client1.config_get(\"maxmemory-policy\"))\n",
    "print(redis_client1.config_get(\"maxmemory-samples\"))\n",
    "\n",
    "#print(redis_client2.config_get(\"maxmemory\"))\n",
    "#print(redis_client2.config_get(\"maxmemory-policy\"))\n",
    "#print(redis_client2.config_get(\"maxmemory-samples\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time: 2018-07-02 11:40:42\n",
      "TrainingResult(timesteps_total=25050, done=None, info={'min_exploration': 0.0006553600000000003, 'max_exploration': 0.4, 'num_target_updates': 17, 'num_steps_trained': 92032, 'num_steps_sampled': 25050, 'sample_throughput': 3812.055, 'train_throughput': 16514.994, 'num_weight_syncs': 61}, episode_reward_mean=-200.0, episode_len_mean=200.0, episodes_total=134, mean_accuracy=None, mean_validation_accuracy=None, mean_loss=None, neg_mean_loss=None, experiment_id='a3fa9ed9ee0e4f479067ec71baa604f6', training_iteration=1, timesteps_this_iter=25050, time_this_iter_s=6.713785171508789, time_total_s=6.713785171508789, pid=4142, date='2018-07-02_11-40-49', timestamp=1530502849, hostname='09a1146da8fb', node_ip='172.17.0.2', config={'dueling': True, 'double_q': True, 'hiddens': [256], 'n_step': 3, 'model': {}, 'gamma': 0.99, 'env_config': {}, 'schedule_max_timesteps': 100000, 'timesteps_per_iteration': 25000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.02, 'target_network_update_freq': 5000, 'random_starts': True, 'buffer_size': 50000, 'prioritized_replay': True, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'clip_rewards': True, 'lr': 0.0002, 'grad_norm_clipping': 40, 'learning_starts': 500, 'sample_batch_size': 50, 'train_batch_size': 128, 'smoothing_num_episodes': 100, 'tf_session_args': {'device_count': {'CPU': 2}, 'log_device_placement': False, 'allow_soft_placement': True, 'gpu_options': {'allow_growth': True}, 'inter_op_parallelism_threads': 1, 'intra_op_parallelism_threads': 1}, 'num_workers': 4, 'num_gpus_per_worker': 0, 'optimizer_class': 'ApexOptimizer', 'optimizer_config': {'max_weight_sync_delay': 400, 'num_replay_buffer_shards': 4, 'debug': False, 'buffer_size': 50000, 'prioritized_replay': True, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'sample_batch_size': 50, 'train_batch_size': 128, 'learning_starts': 500}, 'per_worker_exploration': True, 'worker_side_prioritization': True, 'max_weight_sync_delay': 400, 'env': 'MountainCar-v0'})\n",
      "\n",
      "\n",
      "[ 2 / 500 ] iteration, loop time: 6.72, Avg. Reward: -192.67647\n",
      "[ 3 / 500 ] iteration, loop time: 6.50, Avg. Reward: -194.71\n",
      "[ 4 / 500 ] iteration, loop time: 6.39, Avg. Reward: -159.29\n",
      "[ 5 / 500 ] iteration, loop time: 6.52, Avg. Reward: -148.95\n",
      "[ 6 / 500 ] iteration, loop time: 6.36, Avg. Reward: -153.22\n",
      "[ 7 / 500 ] iteration, loop time: 6.54, Avg. Reward: -143.26\n",
      "[ 8 / 500 ] iteration, loop time: 6.38, Avg. Reward: -128.44\n",
      "[ 9 / 500 ] iteration, loop time: 6.41, Avg. Reward: -124.88\n",
      "[ 10 / 500 ] iteration, loop time: 6.59, Avg. Reward: -124.65\n",
      "[ 11 / 500 ] iteration, loop time: 6.52, Avg. Reward: -122.33\n",
      "[ 12 / 500 ] iteration, loop time: 6.53, Avg. Reward: -121.93\n",
      "[ 13 / 500 ] iteration, loop time: 6.46, Avg. Reward: -118.76\n",
      "[ 14 / 500 ] iteration, loop time: 6.39, Avg. Reward: -114.88\n",
      "[ 15 / 500 ] iteration, loop time: 6.56, Avg. Reward: -113.21\n",
      "[ 16 / 500 ] iteration, loop time: 6.41, Avg. Reward: -112.25\n",
      "[ 17 / 500 ] iteration, loop time: 6.45, Avg. Reward: -115.23\n",
      "[ 18 / 500 ] iteration, loop time: 6.58, Avg. Reward: -117.3\n",
      "[ 19 / 500 ] iteration, loop time: 6.44, Avg. Reward: -113.9\n",
      "[ 20 / 500 ] iteration, loop time: 6.51, Avg. Reward: -108.79\n",
      "[ 21 / 500 ] iteration, loop time: 6.47, Avg. Reward: -108.92\n",
      "[ 22 / 500 ] iteration, loop time: 6.42, Avg. Reward: -110.12\n",
      "[ 23 / 500 ] iteration, loop time: 6.49, Avg. Reward: -106.93\n",
      "[ 24 / 500 ] iteration, loop time: 6.44, Avg. Reward: -108.36\n",
      "[ 25 / 500 ] iteration, loop time: 6.38, Avg. Reward: -108.68\n",
      "[ 26 / 500 ] iteration, loop time: 6.38, Avg. Reward: -106.32\n",
      "[ 27 / 500 ] iteration, loop time: 6.36, Avg. Reward: -107.87\n",
      "[ 28 / 500 ] iteration, loop time: 6.42, Avg. Reward: -109.23\n",
      "[ 29 / 500 ] iteration, loop time: 6.42, Avg. Reward: -108.47\n",
      "[ 30 / 500 ] iteration, loop time: 6.42, Avg. Reward: -109.18\n",
      "[ 31 / 500 ] iteration, loop time: 6.44, Avg. Reward: -112.08\n",
      "[ 32 / 500 ] iteration, loop time: 6.44, Avg. Reward: -109.73\n",
      "[ 33 / 500 ] iteration, loop time: 6.39, Avg. Reward: -106.66\n",
      "[ 34 / 500 ] iteration, loop time: 6.40, Avg. Reward: -110.76\n",
      "[ 35 / 500 ] iteration, loop time: 6.32, Avg. Reward: -109.33\n",
      "[ 36 / 500 ] iteration, loop time: 6.33, Avg. Reward: -107.99\n",
      "[ 37 / 500 ] iteration, loop time: 6.58, Avg. Reward: -110.97\n",
      "[ 38 / 500 ] iteration, loop time: 6.35, Avg. Reward: -111.96\n",
      "[ 39 / 500 ] iteration, loop time: 6.41, Avg. Reward: -110.67\n",
      "[ 40 / 500 ] iteration, loop time: 6.56, Avg. Reward: -109.89\n",
      "[ 41 / 500 ] iteration, loop time: 6.39, Avg. Reward: -102.47\n",
      "[ 42 / 500 ] iteration, loop time: 6.44, Avg. Reward: -106.43\n",
      "[ 43 / 500 ] iteration, loop time: 6.46, Avg. Reward: -113.75\n",
      "[ 44 / 500 ] iteration, loop time: 6.36, Avg. Reward: -109.24\n",
      "[ 45 / 500 ] iteration, loop time: 6.48, Avg. Reward: -107.4\n",
      "[ 46 / 500 ] iteration, loop time: 6.49, Avg. Reward: -106.86\n",
      "[ 47 / 500 ] iteration, loop time: 6.32, Avg. Reward: -107.37\n",
      "[ 48 / 500 ] iteration, loop time: 6.48, Avg. Reward: -107.58\n",
      "[ 49 / 500 ] iteration, loop time: 6.52, Avg. Reward: -107.98\n",
      "[ 50 / 500 ] iteration, loop time: 6.45, Avg. Reward: -111.62\n",
      "[ 51 / 500 ] iteration, loop time: 6.51, Avg. Reward: -107.54\n",
      "[ 52 / 500 ] iteration, loop time: 6.49, Avg. Reward: -104.28\n",
      "[ 53 / 500 ] iteration, loop time: 6.45, Avg. Reward: -108.56\n",
      "[ 54 / 500 ] iteration, loop time: 6.49, Avg. Reward: -107.33\n",
      "[ 55 / 500 ] iteration, loop time: 6.34, Avg. Reward: -107.58\n",
      "[ 56 / 500 ] iteration, loop time: 6.54, Avg. Reward: -105.48\n",
      "[ 57 / 500 ] iteration, loop time: 6.46, Avg. Reward: -102.19\n",
      "[ 58 / 500 ] iteration, loop time: 6.51, Avg. Reward: -104.6\n",
      "[ 59 / 500 ] iteration, loop time: 6.56, Avg. Reward: -104.46\n",
      "[ 60 / 500 ] iteration, loop time: 6.39, Avg. Reward: -105.39\n",
      "[ 61 / 500 ] iteration, loop time: 6.54, Avg. Reward: -106.25\n",
      "[ 62 / 500 ] iteration, loop time: 6.54, Avg. Reward: -104.51\n",
      "[ 63 / 500 ] iteration, loop time: 6.48, Avg. Reward: -104.91\n",
      "[ 64 / 500 ] iteration, loop time: 6.47, Avg. Reward: -107.44\n",
      "[ 65 / 500 ] iteration, loop time: 6.56, Avg. Reward: -107.68\n",
      "[ 66 / 500 ] iteration, loop time: 6.38, Avg. Reward: -107.6\n",
      "[ 67 / 500 ] iteration, loop time: 6.48, Avg. Reward: -107.15\n",
      "[ 68 / 500 ] iteration, loop time: 6.50, Avg. Reward: -107.42\n",
      "[ 69 / 500 ] iteration, loop time: 6.52, Avg. Reward: -109.28\n",
      "[ 70 / 500 ] iteration, loop time: 6.53, Avg. Reward: -109.94\n",
      "[ 71 / 500 ] iteration, loop time: 6.41, Avg. Reward: -109.35\n",
      "[ 72 / 500 ] iteration, loop time: 6.49, Avg. Reward: -108.06\n",
      "[ 73 / 500 ] iteration, loop time: 6.63, Avg. Reward: -109.12\n",
      "[ 74 / 500 ] iteration, loop time: 6.38, Avg. Reward: -111.06\n",
      "[ 75 / 500 ] iteration, loop time: 6.60, Avg. Reward: -109.78\n",
      "[ 76 / 500 ] iteration, loop time: 6.46, Avg. Reward: -108.49\n",
      "[ 77 / 500 ] iteration, loop time: 6.57, Avg. Reward: -109.55\n",
      "[ 78 / 500 ] iteration, loop time: 6.62, Avg. Reward: -110.58\n",
      "[ 79 / 500 ] iteration, loop time: 6.39, Avg. Reward: -110.27\n",
      "[ 80 / 500 ] iteration, loop time: 6.52, Avg. Reward: -110.04\n",
      "[ 81 / 500 ] iteration, loop time: 6.42, Avg. Reward: -109.3\n",
      "[ 82 / 500 ] iteration, loop time: 6.43, Avg. Reward: -110.32\n",
      "[ 83 / 500 ] iteration, loop time: 6.44, Avg. Reward: -112.78\n",
      "[ 84 / 500 ] iteration, loop time: 6.51, Avg. Reward: -110.49\n",
      "[ 85 / 500 ] iteration, loop time: 6.42, Avg. Reward: -110.48\n",
      "[ 86 / 500 ] iteration, loop time: 6.45, Avg. Reward: -111.45\n",
      "[ 87 / 500 ] iteration, loop time: 6.44, Avg. Reward: -108.28\n",
      "[ 88 / 500 ] iteration, loop time: 6.48, Avg. Reward: -108.47\n",
      "[ 89 / 500 ] iteration, loop time: 6.54, Avg. Reward: -108.73\n",
      "[ 90 / 500 ] iteration, loop time: 6.51, Avg. Reward: -108.9\n",
      "[ 91 / 500 ] iteration, loop time: 6.55, Avg. Reward: -109.37\n",
      "[ 92 / 500 ] iteration, loop time: 6.40, Avg. Reward: -109.54\n",
      "[ 93 / 500 ] iteration, loop time: 6.37, Avg. Reward: -109.44\n",
      "[ 94 / 500 ] iteration, loop time: 6.54, Avg. Reward: -110.43\n",
      "[ 95 / 500 ] iteration, loop time: 6.47, Avg. Reward: -110.79\n",
      "[ 96 / 500 ] iteration, loop time: 6.42, Avg. Reward: -112.54\n",
      "[ 97 / 500 ] iteration, loop time: 6.51, Avg. Reward: -111.57\n",
      "[ 98 / 500 ] iteration, loop time: 6.49, Avg. Reward: -112.12\n",
      "[ 99 / 500 ] iteration, loop time: 6.46, Avg. Reward: -108.52\n",
      "[ 100 / 500 ] iteration, loop time: 6.40, Avg. Reward: -111.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 101 / 500 ] iteration, loop time: 6.42, Avg. Reward: -111.61\n",
      "[ 102 / 500 ] iteration, loop time: 6.70, Avg. Reward: -109.92\n",
      "[ 103 / 500 ] iteration, loop time: 6.39, Avg. Reward: -110.23\n",
      "[ 104 / 500 ] iteration, loop time: 6.46, Avg. Reward: -112.68\n",
      "[ 105 / 500 ] iteration, loop time: 6.49, Avg. Reward: -111.01\n",
      "[ 106 / 500 ] iteration, loop time: 6.53, Avg. Reward: -109.95\n",
      "[ 107 / 500 ] iteration, loop time: 6.51, Avg. Reward: -106.63\n",
      "[ 108 / 500 ] iteration, loop time: 6.53, Avg. Reward: -108.78\n",
      "[ 109 / 500 ] iteration, loop time: 6.36, Avg. Reward: -112.51\n",
      "[ 110 / 500 ] iteration, loop time: 6.37, Avg. Reward: -113.8\n",
      "[ 111 / 500 ] iteration, loop time: 6.38, Avg. Reward: -110.11\n",
      "[ 112 / 500 ] iteration, loop time: 6.51, Avg. Reward: -109.35\n",
      "[ 113 / 500 ] iteration, loop time: 6.49, Avg. Reward: -110.11\n",
      "[ 114 / 500 ] iteration, loop time: 6.42, Avg. Reward: -114.45\n",
      "[ 115 / 500 ] iteration, loop time: 6.50, Avg. Reward: -115.72\n",
      "[ 116 / 500 ] iteration, loop time: 6.49, Avg. Reward: -112.77\n",
      "[ 117 / 500 ] iteration, loop time: 6.44, Avg. Reward: -111.19\n",
      "[ 118 / 500 ] iteration, loop time: 6.47, Avg. Reward: -114.06\n",
      "[ 119 / 500 ] iteration, loop time: 6.40, Avg. Reward: -122.49\n",
      "[ 120 / 500 ] iteration, loop time: 6.43, Avg. Reward: -117.26\n",
      "[ 121 / 500 ] iteration, loop time: 6.55, Avg. Reward: -108.84\n",
      "[ 122 / 500 ] iteration, loop time: 6.56, Avg. Reward: -109.67\n",
      "[ 123 / 500 ] iteration, loop time: 6.50, Avg. Reward: -109.59\n",
      "[ 124 / 500 ] iteration, loop time: 6.55, Avg. Reward: -110.42\n",
      "[ 125 / 500 ] iteration, loop time: 6.46, Avg. Reward: -111.66\n",
      "[ 126 / 500 ] iteration, loop time: 6.48, Avg. Reward: -113.06\n",
      "[ 127 / 500 ] iteration, loop time: 6.44, Avg. Reward: -113.14\n",
      "[ 128 / 500 ] iteration, loop time: 6.41, Avg. Reward: -116.73\n",
      "[ 129 / 500 ] iteration, loop time: 6.53, Avg. Reward: -116.54\n",
      "[ 130 / 500 ] iteration, loop time: 6.42, Avg. Reward: -116.97\n",
      "[ 131 / 500 ] iteration, loop time: 6.43, Avg. Reward: -116.93\n",
      "[ 132 / 500 ] iteration, loop time: 6.57, Avg. Reward: -113.2\n",
      "[ 133 / 500 ] iteration, loop time: 6.47, Avg. Reward: -112.01\n",
      "[ 134 / 500 ] iteration, loop time: 6.47, Avg. Reward: -117.42\n",
      "[ 135 / 500 ] iteration, loop time: 6.42, Avg. Reward: -116.03\n",
      "[ 136 / 500 ] iteration, loop time: 6.44, Avg. Reward: -110.02\n",
      "[ 137 / 500 ] iteration, loop time: 6.60, Avg. Reward: -111.27\n",
      "[ 138 / 500 ] iteration, loop time: 6.57, Avg. Reward: -110.72\n",
      "[ 139 / 500 ] iteration, loop time: 6.42, Avg. Reward: -111.83\n",
      "[ 140 / 500 ] iteration, loop time: 6.48, Avg. Reward: -112.53\n",
      "[ 141 / 500 ] iteration, loop time: 6.49, Avg. Reward: -110.43\n",
      "[ 142 / 500 ] iteration, loop time: 6.41, Avg. Reward: -108.66\n",
      "[ 143 / 500 ] iteration, loop time: 6.44, Avg. Reward: -111.87\n",
      "[ 144 / 500 ] iteration, loop time: 6.43, Avg. Reward: -111.78\n",
      "[ 145 / 500 ] iteration, loop time: 6.50, Avg. Reward: -110.38\n",
      "[ 146 / 500 ] iteration, loop time: 6.52, Avg. Reward: -110.02\n",
      "[ 147 / 500 ] iteration, loop time: 6.34, Avg. Reward: -110.8\n",
      "[ 148 / 500 ] iteration, loop time: 6.61, Avg. Reward: -111.37\n",
      "[ 149 / 500 ] iteration, loop time: 6.47, Avg. Reward: -111.45\n",
      "[ 150 / 500 ] iteration, loop time: 6.51, Avg. Reward: -111.06\n",
      "[ 151 / 500 ] iteration, loop time: 6.44, Avg. Reward: -110.42\n",
      "[ 152 / 500 ] iteration, loop time: 6.43, Avg. Reward: -112.67\n",
      "[ 153 / 500 ] iteration, loop time: 6.48, Avg. Reward: -114.49\n",
      "[ 154 / 500 ] iteration, loop time: 6.51, Avg. Reward: -109.64\n",
      "[ 155 / 500 ] iteration, loop time: 6.38, Avg. Reward: -108.46\n",
      "[ 156 / 500 ] iteration, loop time: 6.42, Avg. Reward: -113.76\n",
      "[ 157 / 500 ] iteration, loop time: 6.45, Avg. Reward: -114.17\n",
      "[ 158 / 500 ] iteration, loop time: 6.45, Avg. Reward: -114.71\n",
      "[ 159 / 500 ] iteration, loop time: 6.52, Avg. Reward: -113.57\n",
      "[ 160 / 500 ] iteration, loop time: 6.44, Avg. Reward: -111.57\n",
      "[ 161 / 500 ] iteration, loop time: 6.43, Avg. Reward: -109.89\n",
      "[ 162 / 500 ] iteration, loop time: 6.54, Avg. Reward: -109.13\n",
      "[ 163 / 500 ] iteration, loop time: 6.49, Avg. Reward: -110.74\n",
      "[ 164 / 500 ] iteration, loop time: 6.42, Avg. Reward: -110.1\n",
      "[ 165 / 500 ] iteration, loop time: 6.56, Avg. Reward: -110.05\n",
      "[ 166 / 500 ] iteration, loop time: 6.31, Avg. Reward: -113.23\n",
      "[ 167 / 500 ] iteration, loop time: 6.55, Avg. Reward: -112.58\n",
      "[ 168 / 500 ] iteration, loop time: 6.47, Avg. Reward: -110.69\n",
      "[ 169 / 500 ] iteration, loop time: 6.62, Avg. Reward: -111.24\n",
      "[ 170 / 500 ] iteration, loop time: 6.54, Avg. Reward: -111.2\n",
      "[ 171 / 500 ] iteration, loop time: 6.54, Avg. Reward: -110.87\n",
      "[ 172 / 500 ] iteration, loop time: 6.62, Avg. Reward: -110.17\n",
      "[ 173 / 500 ] iteration, loop time: 6.63, Avg. Reward: -111.9\n",
      "[ 174 / 500 ] iteration, loop time: 6.39, Avg. Reward: -110.02\n",
      "[ 175 / 500 ] iteration, loop time: 6.44, Avg. Reward: -111.09\n",
      "[ 176 / 500 ] iteration, loop time: 6.40, Avg. Reward: -114.02\n",
      "[ 177 / 500 ] iteration, loop time: 6.42, Avg. Reward: -110.49\n",
      "[ 178 / 500 ] iteration, loop time: 6.54, Avg. Reward: -108.84\n",
      "[ 179 / 500 ] iteration, loop time: 6.34, Avg. Reward: -112.38\n",
      "[ 180 / 500 ] iteration, loop time: 6.54, Avg. Reward: -115.66\n",
      "[ 181 / 500 ] iteration, loop time: 6.44, Avg. Reward: -116.92\n",
      "[ 182 / 500 ] iteration, loop time: 6.44, Avg. Reward: -115.28\n",
      "[ 183 / 500 ] iteration, loop time: 6.51, Avg. Reward: -113.54\n",
      "[ 184 / 500 ] iteration, loop time: 6.47, Avg. Reward: -115.55\n",
      "[ 185 / 500 ] iteration, loop time: 6.50, Avg. Reward: -117.15\n",
      "[ 186 / 500 ] iteration, loop time: 6.59, Avg. Reward: -130.59\n",
      "[ 187 / 500 ] iteration, loop time: 6.51, Avg. Reward: -132.11\n",
      "[ 188 / 500 ] iteration, loop time: 6.57, Avg. Reward: -122.12\n",
      "[ 189 / 500 ] iteration, loop time: 6.48, Avg. Reward: -123.55\n",
      "[ 190 / 500 ] iteration, loop time: 6.29, Avg. Reward: -127.67\n",
      "[ 191 / 500 ] iteration, loop time: 6.59, Avg. Reward: -124.93\n",
      "[ 192 / 500 ] iteration, loop time: 6.43, Avg. Reward: -124.13\n",
      "[ 193 / 500 ] iteration, loop time: 6.39, Avg. Reward: -121.68\n",
      "[ 194 / 500 ] iteration, loop time: 6.59, Avg. Reward: -123.49\n",
      "[ 195 / 500 ] iteration, loop time: 6.51, Avg. Reward: -121.81\n",
      "[ 196 / 500 ] iteration, loop time: 6.36, Avg. Reward: -121.05\n",
      "[ 197 / 500 ] iteration, loop time: 6.51, Avg. Reward: -114.6\n",
      "[ 198 / 500 ] iteration, loop time: 6.49, Avg. Reward: -107.63\n",
      "[ 199 / 500 ] iteration, loop time: 6.58, Avg. Reward: -106.22\n",
      "[ 200 / 500 ] iteration, loop time: 6.56, Avg. Reward: -105.03\n",
      "[ 201 / 500 ] iteration, loop time: 6.37, Avg. Reward: -105.64\n",
      "[ 202 / 500 ] iteration, loop time: 6.42, Avg. Reward: -105.72\n",
      "[ 203 / 500 ] iteration, loop time: 6.61, Avg. Reward: -109.36\n",
      "[ 204 / 500 ] iteration, loop time: 6.48, Avg. Reward: -109.92\n",
      "[ 205 / 500 ] iteration, loop time: 6.35, Avg. Reward: -110.85\n",
      "[ 206 / 500 ] iteration, loop time: 6.68, Avg. Reward: -111.44\n",
      "[ 207 / 500 ] iteration, loop time: 6.41, Avg. Reward: -109.1\n",
      "[ 208 / 500 ] iteration, loop time: 6.48, Avg. Reward: -109.75\n",
      "[ 209 / 500 ] iteration, loop time: 6.42, Avg. Reward: -110.36\n",
      "[ 210 / 500 ] iteration, loop time: 6.55, Avg. Reward: -109.85\n",
      "[ 211 / 500 ] iteration, loop time: 6.42, Avg. Reward: -111.79\n",
      "[ 212 / 500 ] iteration, loop time: 6.43, Avg. Reward: -113.7\n",
      "[ 213 / 500 ] iteration, loop time: 6.55, Avg. Reward: -110.51\n",
      "[ 214 / 500 ] iteration, loop time: 6.58, Avg. Reward: -111.1\n",
      "[ 215 / 500 ] iteration, loop time: 6.38, Avg. Reward: -109.92\n",
      "[ 216 / 500 ] iteration, loop time: 6.48, Avg. Reward: -107.93\n",
      "[ 217 / 500 ] iteration, loop time: 6.62, Avg. Reward: -114.71\n",
      "[ 218 / 500 ] iteration, loop time: 6.53, Avg. Reward: -116.27\n",
      "[ 219 / 500 ] iteration, loop time: 6.40, Avg. Reward: -109.59\n",
      "[ 220 / 500 ] iteration, loop time: 6.35, Avg. Reward: -109.36\n",
      "[ 221 / 500 ] iteration, loop time: 6.62, Avg. Reward: -109.51\n",
      "[ 222 / 500 ] iteration, loop time: 6.51, Avg. Reward: -109.53\n",
      "[ 223 / 500 ] iteration, loop time: 6.39, Avg. Reward: -109.32\n",
      "[ 224 / 500 ] iteration, loop time: 6.45, Avg. Reward: -109.78\n",
      "[ 225 / 500 ] iteration, loop time: 6.55, Avg. Reward: -110.73\n",
      "[ 226 / 500 ] iteration, loop time: 6.39, Avg. Reward: -110.16\n",
      "[ 227 / 500 ] iteration, loop time: 6.49, Avg. Reward: -109.33\n",
      "[ 228 / 500 ] iteration, loop time: 6.40, Avg. Reward: -113.15\n",
      "[ 229 / 500 ] iteration, loop time: 6.55, Avg. Reward: -113.63\n",
      "[ 230 / 500 ] iteration, loop time: 6.64, Avg. Reward: -111.49\n",
      "[ 231 / 500 ] iteration, loop time: 6.56, Avg. Reward: -111.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 232 / 500 ] iteration, loop time: 6.51, Avg. Reward: -112.43\n",
      "[ 233 / 500 ] iteration, loop time: 6.56, Avg. Reward: -110.07\n",
      "[ 234 / 500 ] iteration, loop time: 6.48, Avg. Reward: -110.84\n",
      "[ 235 / 500 ] iteration, loop time: 6.45, Avg. Reward: -111.26\n",
      "[ 236 / 500 ] iteration, loop time: 6.47, Avg. Reward: -109.15\n",
      "[ 237 / 500 ] iteration, loop time: 6.55, Avg. Reward: -110.04\n",
      "[ 238 / 500 ] iteration, loop time: 6.64, Avg. Reward: -109.79\n",
      "[ 239 / 500 ] iteration, loop time: 6.35, Avg. Reward: -109.09\n",
      "[ 240 / 500 ] iteration, loop time: 6.46, Avg. Reward: -108.82\n",
      "[ 241 / 500 ] iteration, loop time: 6.48, Avg. Reward: -110.65\n",
      "[ 242 / 500 ] iteration, loop time: 6.44, Avg. Reward: -111.87\n",
      "[ 243 / 500 ] iteration, loop time: 6.46, Avg. Reward: -111.42\n",
      "[ 244 / 500 ] iteration, loop time: 6.54, Avg. Reward: -110.4\n",
      "[ 245 / 500 ] iteration, loop time: 6.39, Avg. Reward: -108.01\n",
      "[ 246 / 500 ] iteration, loop time: 6.50, Avg. Reward: -108.81\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c716e86cca03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_this_iter_s\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/rllib/dqn/dqn.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m                self.config[\"timesteps_per_iteration\"]):\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/rllib/optimizers/apex_optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0msample_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mtime_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_delta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/rllib/optimizers/apex_optimizer.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample_processing\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_tasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompleted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                 \u001b[0msample_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/actors.py\u001b[0m in \u001b[0;36mcompleted\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpending\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpending\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_returns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpending\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mobj_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_ids, num_returns, timeout, worker)\u001b[0m\n\u001b[1;32m   2405\u001b[0m         ready_ids, remaining_ids = worker.plasma_client.wait(object_id_strs,\n\u001b[1;32m   2406\u001b[0m                                                              \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2407\u001b[0;31m                                                              num_returns)\n\u001b[0m\u001b[1;32m   2408\u001b[0m         ready_ids = [ray.local_scheduler.ObjectID(object_id.binary())\n\u001b[1;32m   2409\u001b[0m                      for object_id in ready_ids]\n",
      "\u001b[0;32mplasma.pyx\u001b[0m in \u001b[0;36mpyarrow.plasma.PlasmaClient.wait\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32merror.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/pyarrow_files/pyarrow/compat.py\u001b[0m in \u001b[0;36mfrombytes\u001b[0;34m(o)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print (\"Current Time: {}\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "\n",
    "time_list = [time.time()]\n",
    "time_delta_list = []\n",
    "\n",
    "num_iters = 500\n",
    "save_chkpt = False\n",
    "\n",
    "for i in range(num_iters):\n",
    "    result = agent.train()\n",
    "    \n",
    "    if i == 0:\n",
    "        print(result)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        time_delta = time_list[-1] - time_list[-2]\n",
    "        time_delta_list.append(time_delta)\n",
    "        print(\"[ {} / {} ] iteration, loop time: {:.2f}, Avg. Reward: {}\".format(i+1, num_iters,\n",
    "                                                                            time_delta,                                                                                           \n",
    "                                                                            result.episode_reward_mean)\n",
    "             )\n",
    "\n",
    "\n",
    "        \n",
    "    time_list.append(time.time())\n",
    "    \n",
    "    # save result every 10 training loop\n",
    "    if save_chkpt and (i+1>1) and ((i+1)%10==0):\n",
    "        checkpoint = agent.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)\n",
    "\n",
    "\n",
    "print (\"Current Time: {}\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "print(\"Training done.\\n\")\n",
    "\n",
    "if save_chkpt:\n",
    "    checkpoint = agent.save()\n",
    "    print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created LogSyncer for /root/ray_results/2018-06-29_10-18-54wws10o3e -> None\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Observation shape is (2,)\n",
      "Not using any observation preprocessor.\n",
      "INFO:tensorflow:Restoring parameters from /root/ray_results/2018-06-29_02-55-05mez6ivqi/checkpoint-1560\n",
      "WARNING: Serializing objects of type <class 'ray.rllib.dqn.common.schedules.ConstantSchedule'> by expanding them as dictionaries of their fields. This behavior may be incorrect in some cases.\n"
     ]
    }
   ],
   "source": [
    "trained_config = config.copy()\n",
    "\n",
    "#test_agent = PPOAgent(trained_config, 'CartPole-v0')\n",
    "#test_agent = ApexAgent(config, 'CartPole-v0')\n",
    "test_agent = ApexAgent(config, 'MountainCar-v0')\n",
    "\n",
    "checkpoint = '/root/ray_results/2018-06-29_02-55-05mez6ivqi/checkpoint-1560'\n",
    "test_agent.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play in the gym env #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "-87.0\n"
     ]
    }
   ],
   "source": [
    "#env = gym.make('CartPole-v0')\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    #env.render()\n",
    "    action = test_agent.compute_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "\n",
    "print(cumulative_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
